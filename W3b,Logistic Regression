import pandas as pd
DATA_DIR = "Dataset/Banking_Marketing.csv"
Banking_Marketing_df = pd.read_csv (DATA_DIR, header=0)

print("Print number of rows: {}".format(len(Banking_Marketing_df)))

print("Print number of columns: {}".format(len(Banking_Marketing_df.columns)))
print()

print("Listing all columns: ") 
print(Banking_Marketing_df.head())
print()

print("Display basic statistic of all columns: ")
print(Banking_Marketing_df.describe())
print()

print("Display basic information of the columns: ")
print(Banking_Marketing_df.info())

# Find how many missing values there are in the columns.
print (Banking_Marketing_df.isnull().sum())

# Remove missing values
# [Before 41199 rows x 21 columns]
print("Before remove missing values: [{} rows x {} col]".format(Banking_Marketing_df.shape[0],Banking_Marketing_df.shape[1] ))

Banking_Marketing_df = (Banking_Marketing_df.dropna())
print("After remove missing values: [{} rows x {} col]".format(Banking_Marketing_df.shape[0],Banking_Marketing_df.shape[1] ))
print (Banking_Marketing_df.isnull().sum())

# Print the frequent distribution of the education column.
import matplotlib.pyplot as plt

print(Banking_Marketing_df['education'].value_counts())
Banking_Marketing_df['education'].value_counts().plot(kind='bar') 
plt.show()

# Reduce categories in education column
import numpy as np

# remove illiterate values coz there are only 18 ppl
Banking_Marketing_df['education'].replace(to_replace=['illiterate'], value=np.NAN, inplace=True)

Banking_Marketing_df['education'].replace(to_replace=['basic.4y', 'basic.6y', 'basic.9y'], value='Basic', inplace=True)
print(Banking_Marketing_df['education'].value_counts())

Banking_Marketing_df['education'].value_counts().plot(kind='bar') 
plt.show()

# Read Dataset and import LabelEncoder from sklearn.preprocessing package
from sklearn.preprocessing import LabelEncoder

print (Banking_Marketing_df.head())

# Select Non-Numerical Columns
data_column_category = Banking_Marketing_df.select_dtypes (exclude=[np.number]).columns
print (data_column_category)
print (Banking_Marketing_df[data_column_category].head())

# Remove Missing Data
Banking_Marketing_df = Banking_Marketing_df.dropna()

# Iterate through column to convert to numeric data using LabelEncoder ()
label_encoder = LabelEncoder()
for i in data_column_category:
  Banking_Marketing_df[i] = label_encoder.fit_transform (Banking_Marketing_df[i])

print("Label Encoder Data:")
print(Banking_Marketing_df.head())

# Data Transformation with StandardScaler()
from sklearn import preprocessing

# Check for Missing Data
null_ = Banking_Marketing_df.isna().any()
dtypes = Banking_Marketing_df.dtypes
info = pd.concat ([null_,dtypes], axis = 1, keys = ['Null', 'type'])
print(info) # This is different way of viewing data

std_scale = preprocessing.StandardScaler().fit_transform (Banking_Marketing_df)
scaled_frame = pd.DataFrame (std_scale, columns = Banking_Marketing_df.columns)
print (scaled_frame.head())

# Data Transformation with MinMax Scaler Method
minmax_scale = preprocessing.MinMaxScaler().fit_transform (Banking_Marketing_df)
scaled_frame = pd.DataFrame (minmax_scale, columns = Banking_Marketing_df.columns)
print (scaled_frame.head())

# Checking the Number of Levels in Categorical Variable
levels = len (pd.value_counts(Banking_Marketing_df['education']))
print ('There are {} levels in the education column'.format (levels))

df_dummies = pd.get_dummies(Banking_Marketing_df, drop_first=True)
print ('There are {} columns in df_dummies'.format (df_dummies.shape[1]))

# Shuffle Rows Prior to Splitting Data into Features (X) and Outcome (Y)
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

df_shuffled = shuffle (df_dummies, random_state=42) #random_state = 0, meaning no shuffling

DV = 'y' # DV => Dependent variable

x = df_shuffled.drop (DV, axis=1) # axis=1, drop the DV column, don't axis = 0
y = df_shuffled [DV] # this our depenedent variable?

# 20% for training, The rest for testing
# random_state = 42 is used to select the 20% of the data set for testing randomly
x_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.20, random_state=42)

print (x_train.head ())

from sklearn.linear_model import LogisticRegression
LRmodel=LogisticRegression()

LRmodel.fit(x_train, y_train)

LR_pred=LRmodel.predict(x_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, LR_pred)

# Let's see what's the predicted value
LR_pred

# Import libraries to generate confusion matrix
from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, LR_pred)
print(confusion_matrix)

predicted_class = LRmodel.predict(x_test)
# in logistic regression, prediction based on classification
# in linear regression, prediction based on numerical variable

# another way to create a confusion matrix
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd

# Assuming you have already defined 'y_test' and 'predicted_class'

# Create the confusion matrix
cm = confusion_matrix(y_test, predicted_class)

# Create a DataFrame from the confusion matrix
cm = pd.DataFrame(cm, columns=['Predicted No', 'Predicted Yes'], index=['Actual No', 'Actual Yes'])

# Calculate row and column totals
cm['Total'] = cm.sum(axis=1)
cm.loc['Total'] = cm.sum()

# Display the confusion matrix
print("Confusion matrix:")
print(cm)


# Import libraries to print classification report
from sklearn.metrics import classification_report
print("\nClassification report:")
print(classification_report (y_test, predicted_class))

# Performance Metrics
from sklearn import metrics
import numpy as np
metrics_df = pd.DataFrame ({'Metric':
['MAE',
'MSE',
'RMSE',
'R-Squared'], 'Value':
[metrics.mean_absolute_error(y_test, LR_pred),
metrics.mean_squared_error (y_test, LR_pred),
np.sqrt (metrics.mean_squared_error (y_test, LR_pred)),
metrics.explained_variance_score (y_test, LR_pred)]}).round(3)
print(metrics_df)